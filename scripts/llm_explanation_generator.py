# =============================================================================
# LLM Explanation Generator for Cancer Detection XAI
# Purpose: Generate layman-friendly explanations using LLaMA 3.2 for XAI outputs
# Environment: Python with requests library
# Date: October 2023
# =============================================================================

# Import Libraries
import json
import requests
import time

# =============================================================================
# Configuration
# Set the path to the XAI output file generated by the test script
# =============================================================================

XAI_OUTPUT_FILE = "outputs/xai_output.json"  # Path to the JSON file from test_with_h5.py

# Hugging Face Router chat-completions endpoint and model configuration
API_URL = "https://router.huggingface.co/v1/chat/completions"
ROUTER_MODEL = os.getenv("HF_ROUTER_MODEL", "meta-llama/Llama-3.1-8B-Instruct")
HUGGING_FACE_API_TOKEN = os.getenv("HF_TOKEN")

if not HUGGING_FACE_API_TOKEN:
    raise EnvironmentError(
        "HF_TOKEN environment variable is not set. "
        "Visit https://huggingface.co/settings/tokens to create a token and export it "
        "before running this script."
    )

# =============================================================================
# Functions
# Handle LLM API calls and explanation generation
# =============================================================================

def get_llm_explanation(pred_class, class_name, xai_insights, confidence):
    """
    Use Hugging Face's Inference API to get a layman-friendly explanation from LLaMA 3.2-3B-Instruct.
    
    Args:
        pred_class (int): Predicted class index.
        class_name (str): Name of the predicted class (e.g., "Brain Cancer" or "ALL").
        xai_insights (str): Summary of XAI findings.
        confidence (float): Model confidence in percentage.
    
    Returns:
        str: Layman-friendly explanation.
    """
    parts = class_name.split()
    if len(parts) > 1:
        location = parts[0].lower()
        tumor_type = parts[1].lower()
    else:
        location = "the affected area"
        tumor_type = class_name.lower()

    tumor_label = f"{tumor_type} tumor" if tumor_type not in ["all"] else f"{tumor_type} condition"

    tumor_location = "in the middle of the body"
    if "brain" in class_name.lower():
        tumor_location = "in the center of the brain" if "brain center" in xai_insights.lower() else "in the brain"
    elif "breast" in class_name.lower():
        tumor_location = "in the breast tissue"
    elif "lung" in class_name.lower():
        tumor_location = "in the lung tissue"
    elif "colon" in class_name.lower():
        tumor_location = "in the colon tissue"
    elif "cervical" in class_name.lower():
        tumor_location = "in the cervical tissue"
    elif "kidney" in class_name.lower():
        tumor_location = "in the kidney tissue"
    elif "oral" in class_name.lower():
        tumor_location = "in the oral cavity"
    elif "lymphoma" in class_name.lower():
        tumor_location = "in the lymphatic system"

    medical_context = f"itâ€™s a common {tumor_type} {('tumor' if tumor_type not in ['all'] else 'condition')}" if tumor_type not in ["all"] else f"itâ€™s a type of blood cancer involving abnormal lymphocytes"

    prompt = (f"Explain in simple, friendly, conversational terms why an AI predicted {class_name} (specifically {location} "
              f"{tumor_label}) for a patientâ€™s medical image "
              f"with {confidence:.1f}% confidence. Mention the tumorâ€™s location ({tumor_location}, "
              f"based on the medical context). Include basic medical context ({medical_context}) "
              f"and keep it short, clear, and positive for non-experts, with an optional emoji for reassurance like ðŸ˜Š. "
              f"Avoid adding fictional names, tools, studies, or unnecessary details. "
              f"For doctors, add a concise technical note in parentheses about the XAI focus (Grad-CAM, Saliency Map, LIME).")

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {HUGGING_FACE_API_TOKEN}",
    }

    payload = {
        "model": ROUTER_MODEL,
        "messages": [
            {
                "role": "system",
                "content": (
                    "You translate XAI findings from a cancer-detection model into short, reassuring "
                    "patient explanations plus a concise technical aside for clinicians."
                ),
            },
            {
                "role": "user",
                "content": prompt,
            },
        ],
        "temperature": 0.7,
        "top_p": 0.9,
        "max_tokens": 250,
    }

    try:
        response = requests.post(API_URL, headers=headers, json=payload, timeout=30)
        response.raise_for_status()
    except requests.exceptions.RequestException as exc:
        print(f"Error calling Hugging Face Router API ({API_URL}): {exc}")
        return "Sorry, I couldnâ€™t generate an explanation right now."

    result = response.json()
    try:
        content = result["choices"][0]["message"]["content"]
    except (KeyError, IndexError, TypeError):
        print(f"Unexpected response format: {result}")
        return "Sorry, the LLM response was malformed."
    return content.strip()

def load_xai_data(file_path):
    try:
        with open(file_path, 'r') as f:
            data = json.load(f)
            print("Loaded XAI data:", data)
            return data
    except FileNotFoundError:
        print(f"Error: Could not find {file_path}. Ensure the test script ran and saved the file.")
        return None
    except json.JSONDecodeError:
        print(f"Error: Invalid JSON in {file_path}. Check the file format.")
        return None

# =============================================================================
# Main Execution
# Load XAI data list and generate explanations for each entry
# =============================================================================

def main():
    print("Starting LLM Explanation Generator...")
    
    try:
        with open(XAI_OUTPUT_FILE, 'r') as f:
            xai_data_list = json.load(f)
            if not isinstance(xai_data_list, list):
                xai_data_list = [xai_data_list]
    except FileNotFoundError:
        print(f"Error: Could not find {XAI_OUTPUT_FILE}. Ensure the test script ran and saved the file.")
        return
    except json.JSONDecodeError:
        print(f"Error: Invalid JSON in {XAI_OUTPUT_FILE}. Check the file format.")
        return
    
    for xai_data in xai_data_list:
        pred_class = xai_data["predicted_class"]
        class_name = xai_data["class_name"]
        confidence = xai_data["confidence"]
        xai_insights = xai_data["xai_insights"]
        
        explanation = get_llm_explanation(pred_class, class_name, xai_insights, confidence)
        print(f"\nExplanation for Image {xai_data.get('image_index', 'Unknown')}:")
        print("Laymanâ€™s Explanation:", explanation)

if __name__ == "__main__":
    main()
# =============================================================================
